{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Colab data file preparation\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "!cp drive/MyDrive/Predictive_Maintenence_Fault_Detection/predictive-maintenance-platform/data_processor.py .\n",
        "!cp drive/MyDrive/Predictive_Maintenence_Fault_Detection/predictive-maintenance-platform/visualizer_analyzer.py .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzGRgs7JQqds",
        "outputId": "47d95dd7-f703-44e2-b1ad-dc0c3bd843c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabnanny import verbose\n",
        "from data_processor import DataProcessor\n",
        "from visualizer_analyzer import DataAV\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load all datasets\n",
        "DS_001 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS01-005.h5\"\n",
        "DS_002 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS02-006.h5\"\n",
        "DS_003 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS03-012.h5\"\n",
        "DS_004 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS04.h5\"\n",
        "DS_005 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS05.h5\"\n",
        "DS_006 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS06.h5\"\n",
        "DS_007 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS07.h5\"\n",
        "DS_008 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS08a-009.h5\"\n",
        "DS_009 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS08c-008.h5\"\n",
        "DS_010 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS08d-010.h5\""
      ],
      "metadata": {
        "id": "Rwp9tyDxtYor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data set and prepare data frames\n",
        "pros = DataProcessor()\n",
        "pros.load_hdf5_to_numpy_arr(DS_004)"
      ],
      "metadata": {
        "id": "DdRX8ShGxYUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a visualizer class. This is useful for understanding the data we work with\n",
        "# it will determine feature extraction and model architecture to some extend\n",
        "vs_an = DataAV(pros.df_aux,pros.df_x_s,pros.x_s_var_names,pros.df_w,pros.df_t,pros.t_var_names,pros.w_var_names,pros.df_ts)"
      ],
      "metadata": {
        "id": "UIaEEsE8xiHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Data"
      ],
      "metadata": {
        "id": "aeiWCqOXxx0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate all visualization graphs and plots necessary for understanding data.\n",
        "\n",
        "vs_an.get_engine_units_in_dataset()\n",
        "vs_an.plot_flight_classes()\n",
        "vs_an.show_engine_health_parameter_stats()\n",
        "vs_an.generate_engine_health_parameter_graphs()\n",
        "vs_an.generate_hpt_eff_over_cycles_all_engines()\n",
        "vs_an.generate_sensor_readings_graphs_single_unit(1)\n",
        "vs_an.generate_sensor_readings_graphs_single_unit_single_cycle(1, 2)\n",
        "vs_an.plot_health_states_for_all_engines()\n",
        "vs_an.generate_flight_profle_single_unit_single_cycle(1, 2)\n",
        "vs_an.generate_flight_envelope()\n",
        "vs_an.generate_kde_estimations_of_flight_profile()"
      ],
      "metadata": {
        "id": "pT-0Jxt-xxLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tf_data_for_model(window, horizon, train_split, batch_size, buffer_size):\n",
        "    \"\"\"\n",
        "    Genearate the tf dataset with proper dimensionality and shape given input parameters.\n",
        "    This dataset will split into training and validation subsets.\n",
        "    \"\"\"\n",
        "\n",
        "    x_train, y_train = pros.custom_ts_multi_data_prep(\n",
        "        pros.w, pros.y_rul, 0, train_split, window, horizon\n",
        "    )\n",
        "\n",
        "    x_vali, y_vali = pros.custom_ts_multi_data_prep(\n",
        "        pros.w, pros.y_rul, train_split, None, window, horizon\n",
        "    )\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
        "\n",
        "    val_data = tf.data.Dataset.from_tensor_slices((x_vali, y_vali))\n",
        "    val_data = val_data.batch(batch_size).repeat()\n"
      ],
      "metadata": {
        "id": "6dfwTnSHyV64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pdm_model():\n",
        "\n",
        "    \"\"\"\n",
        "    Build the DNN model with its layers that will be used for RUL preditcions.\n",
        "    \"\"\"\n",
        "\n",
        "    model = tf.keras.models.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Bidirectional(\n",
        "                tf.keras.layers.LSTM(200, return_sequences=True), input_shape=x_train.shape[-2:]\n",
        "            ),\n",
        "            tf.keras.layers.Dense(20, activation=\"tanh\"),\n",
        "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)),\n",
        "            tf.keras.layers.Dense(20, activation=\"tanh\"),\n",
        "            tf.keras.layers.Dense(20, activation=\"tanh\"),\n",
        "            tf.keras.layers.Dropout(0.25),\n",
        "            tf.keras.layers.Dense(1),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    model.summary()"
      ],
      "metadata": {
        "id": "buTmZvg4yZto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0a65UmkH0bD2"
      },
      "outputs": [],
      "source": [
        "def train_pdm_model(epochs, steps_per_epoch, validation_steps, verbose):\n",
        "    \"\"\"\n",
        "    Train the generate model based on provided epochs and steps.\n",
        "    This function will also automatically generate the plot for loss history.\n",
        "    \"\"\"\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    history = model.fit(\n",
        "        x=train_data, epochs=epochs, steps_per_epoch = steps_per_epoch,validation_data= val_data, validation_steps = validation_steps, verbose = verbose\n",
        "    )\n",
        "    vs_an.plot_training_results_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model dataset building parameters\n",
        "hist_window = 48\n",
        "horizon = 1\n",
        "train_split = 4906636\n",
        "batch_size = 256\n",
        "buffer_size = 150\n",
        "\n",
        "generate_tf_data_for_model(hist_window, horizon, train_split, batch_size, buffer_size)"
      ],
      "metadata": {
        "id": "hpw8ogPKNTWf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_pdm_model()"
      ],
      "metadata": {
        "id": "WqV0CmoCNWFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "epochs = 150\n",
        "steps = 100\n",
        "validation_steps = 50\n",
        "verbose = 1\n",
        "\n",
        "train_pdm_model(epochs,steps,validation_steps,verbose)"
      ],
      "metadata": {
        "id": "gQ-QzbpDqqoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_results_history(history)"
      ],
      "metadata": {
        "id": "P0tjCzD3zQDr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
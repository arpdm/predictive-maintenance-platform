{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a65UmkH0bD2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Project Name: PdM Model for NASA Jet Engines\n",
        "    Data: 10/9/2022\n",
        "    Author: Arpi Derm\n",
        "    Description: This model aims to use PdM model and RUL to predict failures in point time T in near future.\n",
        "                 This specific model is built and trained for the NASA's PCoE Turbofan Engine Degradation (Run-To-Failure) dataset.\n",
        "    Dataset Description: \n",
        "                Dataset description along with its variable can be found in the dataset paper writtent by \n",
        "                Manuel Arias Chao,Chetan Kulkarni, Kai Goebel and Olga Fink. https://dx.doi.org/10.3390/data6010005\n",
        "    Variable Name Descriptions:\n",
        "            w = Scenario-descriptor operating conditions (inputs to system model)\n",
        "            x_s = sensor signal measurements (physical properties)\n",
        "            x_v = virtual sensor signals\n",
        "            t = engine health parameters \n",
        "            y_rul = target output Y - RUL (Remaning Useful Life) of engine unit\n",
        "            aux = Auxiliaryl data such as cycle count, unit id, ...\n",
        "\"\"\"\n",
        "\n",
        "from tabnanny import verbose\n",
        "from data_processor import DataProcessor\n",
        "from visualizer_analyzer import DataAV\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load all datasets\n",
        "DS_001 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS01-005.h5\"\n",
        "DS_002 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS02-006.h5\"\n",
        "DS_003 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS03-012.h5\"\n",
        "DS_004 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS04.h5\"\n",
        "DS_005 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS05.h5\"\n",
        "DS_006 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS06.h5\"\n",
        "DS_007 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS07.h5\"\n",
        "DS_008 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS08a-009.h5\"\n",
        "DS_009 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS08c-008.h5\"\n",
        "DS_010 = \"/content/drive/MyDrive/Predictive_Maintenence_Fault_Detection/data_set/N-CMAPSS_DS08d-010.h5\"\n",
        "\n",
        "\n",
        "class EngineRUL:\n",
        "    def __init__(self, dataset):\n",
        "\n",
        "        # Load data set and prepare data frames\n",
        "        self.pros = DataProcessor()\n",
        "        self.pros.load_hdf5_to_numpy_arr(dataset)\n",
        "\n",
        "        # Create a visualizer class. This is useful for understanding the data we work with\n",
        "        # it will determine feature extraction and model architecture to some extend\n",
        "        self.vs_an = DataAV(\n",
        "            self.pros.df_aux,\n",
        "            self.pros.df_x_s,\n",
        "            self.pros.x_s_var_names,\n",
        "            self.pros.df_w,\n",
        "            self.pros.df_t,\n",
        "            self.pros.t_var_names,\n",
        "            self.pros.w_var_names,\n",
        "            self.pros.df_ts,\n",
        "        )\n",
        "\n",
        "    def visualize_data(self):\n",
        "        \"\"\"\n",
        "        Generate all visualization graphs and plots necessary for understanding data.\n",
        "        \"\"\"\n",
        "\n",
        "        self.vs_an.get_engine_units_in_dataset()\n",
        "        self.vs_an.plot_flight_classes()\n",
        "        self.vs_an.show_engine_health_parameter_stats()\n",
        "        self.vs_an.generate_engine_health_parameter_graphs()\n",
        "        self.vs_an.generate_hpt_eff_over_cycles_all_engines()\n",
        "        self.vs_an.generate_sensor_readings_graphs_single_unit(1)\n",
        "        self.vs_an.generate_sensor_readings_graphs_single_unit_single_cycle(1, 2)\n",
        "        self.vs_an.plot_health_states_for_all_engines()\n",
        "        self.vs_an.generate_flight_profle_single_unit_single_cycle(1, 2)\n",
        "        self.vs_an.generate_flight_envelope()\n",
        "        self.vs_an.generate_kde_estimations_of_flight_profile()\n",
        "\n",
        "    def generate_tf_data_for_model(self, window, horizon, train_split, batch_size, buffer_size):\n",
        "        \"\"\"\n",
        "        Genearate the tf dataset with proper dimensionality and shape given input parameters.\n",
        "        This dataset will split into training and validation subsets.\n",
        "        \"\"\"\n",
        "\n",
        "        self.x_train, self.y_train = self.pros.custom_ts_multi_data_prep(\n",
        "            self.pros.w, self.pros.y_rul, 0, train_split, window, horizon\n",
        "        )\n",
        "\n",
        "        self.x_vali, self.y_vali = self.pros.custom_ts_multi_data_prep(\n",
        "            self.pros.w, self.pros.y_rul, train_split, None, window, horizon\n",
        "        )\n",
        "\n",
        "        self.train_data = tf.data.Dataset.from_tensor_slices((self.x_train, self.y_train))\n",
        "        self.train_data = self.train_data.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
        "\n",
        "        self.val_data = tf.data.Dataset.from_tensor_slices((self.x_vali, self.y_vali))\n",
        "        self.val_data = self.val_data.batch(batch_size).repeat()\n",
        "\n",
        "    def generate_pdm_model(self):\n",
        "        \"\"\"\n",
        "        Build the DNN model with its layers that will be used for RUL preditcions.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model = tf.keras.models.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.Bidirectional(\n",
        "                    tf.keras.layers.LSTM(200, return_sequences=True), input_shape=self.x_train.shape[-2:]\n",
        "                ),\n",
        "                tf.keras.layers.Dense(20, activation=\"tanh\"),\n",
        "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)),\n",
        "                tf.keras.layers.Dense(20, activation=\"tanh\"),\n",
        "                tf.keras.layers.Dense(20, activation=\"tanh\"),\n",
        "                tf.keras.layers.Dropout(0.25),\n",
        "                tf.keras.layers.Dense(1),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.model.compile(optimizer=\"adam\", loss=\"rmse\")\n",
        "        self.model.summary()\n",
        "\n",
        "    def train_pdm_model(self, epochs=100, steps_per_epoch=100, validation_steps=50, verbose=1):\n",
        "        \"\"\"\n",
        "        Train the generate model based on provided epochs and steps.\n",
        "        This function will also automatically generate the plot for loss history.\n",
        "        \"\"\"\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "        self.history = self.model.fit(\n",
        "            self.train_data, epochs, steps_per_epoch, self.val_data, validation_steps, verbose\n",
        "        )\n",
        "        self.vs_an.plot_training_results_history(self.history)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Model dataset building parameters\n",
        "    hist_window = 48\n",
        "    horizon = 1\n",
        "    train_split = 4906636\n",
        "    batch_size = 256\n",
        "    buffer_size = 150\n",
        "\n",
        "    # Training parameters\n",
        "    epochs = 150\n",
        "    steps = 100\n",
        "    validation_steps = 50\n",
        "    verbose = 1\n",
        "\n",
        "    e_rul = EngineRUL(DS_001)\n",
        "    e_rul.visualize_data()\n",
        "    e_rul.generate_tf_data_for_model(hist_window, horizon, train_split)\n",
        "    e_rul.generate_pdm_model(epochs, steps, validation_steps, verbose)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
